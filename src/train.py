#!/usr/bin/env python3
import gymnasium as gym
import torch
import numpy as np
import random
import yaml
import argparse
import os
from collections import deque

from models.dqn import DQNAgent
from models.utils import create_env, setup_tensorboard, evaluate_agent

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

def train(config):
    """è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = create_env()
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    
    print(f"ç¯å¢ƒ: {config['env_name']}")
    print(f"çŠ¶æ€ç©ºé—´: {state_size}, åŠ¨ä½œç©ºé—´: {action_size}")
    
    agent = DQNAgent(state_size, action_size, config)
    
    # è®¾ç½®TensorBoard
    writer = setup_tensorboard()
    
    # è®­ç»ƒå‚æ•°
    scores = []
    recent_scores = deque(maxlen=100)
    best_score = -float('inf')
    
    print("å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    
    for episode in range(config['episodes']):
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        episode_loss = 0
        episode_q_values = 0
        loss_count = 0
        
        for step in range(config['max_steps']):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # å­˜å‚¨ç»éªŒ
            agent.remember(state, action, reward, next_state, done)
            
            # ç»éªŒå›æ”¾
            loss, q_value = agent.replay()
            if loss > 0:
                episode_loss += loss
                episode_q_values += q_value
                loss_count += 1
            
            total_reward += reward
            state = next_state
            steps += 1
            
            if done:
                break
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        agent.update_target_network(hard=False)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        avg_loss = episode_loss / loss_count if loss_count > 0 else 0
        avg_q_value = episode_q_values / loss_count if loss_count > 0 else 0
        
        scores.append(total_reward)
        recent_scores.append(total_reward)
        mean_recent_score = np.mean(recent_scores)
        
        # è®°å½•åˆ°TensorBoard
        writer.add_scalar('Training/Score', total_reward, episode)
        writer.add_scalar('Training/Average_Score_100', mean_recent_score, episode)
        writer.add_scalar('Training/Steps', steps, episode)
        writer.add_scalar('Training/Loss', avg_loss, episode)
        writer.add_scalar('Training/Epsilon', agent.epsilon, episode)
        writer.add_scalar('Training/Q_Value', avg_q_value, episode)
        writer.add_scalar('Training/Memory_Size', len(agent.memory), episode)
        
        # å®šæœŸè¯„ä¼°
        if episode % config['eval_every'] == 0:
            eval_env = create_env()
            mean_score, std_score = evaluate_agent(agent, eval_env, n_episodes=5)
            eval_env.close()
            
            writer.add_scalar('Evaluation/Mean_Score', mean_score, episode)
            writer.add_scalar('Evaluation/Std_Score', std_score, episode)
            
            print(f"è¯„ä¼°å›åˆ {episode}: å¹³å‡å¾—åˆ† = {mean_score:.2f} Â± {std_score:.2f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if total_reward > best_score:
            best_score = total_reward
            agent.save('models/best_model.pth')
            writer.add_scalar('Training/Best_Score', best_score, episode)
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if episode % config['save_every'] == 0:
            agent.save(f'models/checkpoint_{episode}.pth')
        
        # æ‰“å°è¿›åº¦
        if episode % 10 == 0:
            print(f"å›åˆ {episode:4d}/{config['episodes']} | "
                  f"å¾—åˆ†: {total_reward:7.2f} | "
                  f"å¹³å‡å¾—åˆ†: {mean_recent_score:7.2f} | "
                  f"Îµ: {agent.epsilon:.3f} | "
                  f"æ­¥æ•°: {steps:3d} | "
                  f"è®°å¿†: {len(agent.memory):5d}")
        
        # æ£€æŸ¥æ˜¯å¦è§£å†³ç¯å¢ƒ
        if mean_recent_score >= 200 and len(recent_scores) == 100:
            print(f"ğŸ‰ ç¯å¢ƒåœ¨ {episode} å›åˆè§£å†³!")
            agent.save('models/solved_model.pth')
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save('models/final_model.pth')
    env.close()
    writer.close()
    
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³å¾—åˆ†: {best_score:.2f}")
    print(f"æœ€å100å›åˆå¹³å‡å¾—åˆ†: {np.mean(list(recent_scores)[-100:]):.2f}")
    
    return scores

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒDQNæ™ºèƒ½ä½“ç©LunarLander')
    parser.add_argument('--config', type=str, default='configs/default.yaml', 
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--seed', type=int, default=42, 
                       help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºç›®å½•
    os.makedirs('models', exist_ok=True)
    os.makedirs('runs', exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒ
    scores = train(config)

if __name__ == "__main__":
    main()